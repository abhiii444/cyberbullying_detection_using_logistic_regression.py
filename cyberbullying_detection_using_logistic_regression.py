# -*- coding: utf-8 -*-
"""Cyberbullying Detection Using Logistic Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mASdYGspnkQc2cSZaEKR-5ZspRdULszr
"""

import numpy as np
import pandas as pd
import re
import string
import nltk
from nltk.corpus import stopwords
from tqdm import tqdm
import nltk

# Download all required NLTK resources
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

tqdm.pandas()

df = pd.read_csv('/content/drive/MyDrive/DATASET/cyberbullying_tweets.csv')
df

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Set up figure
unique_labels = df['cyberbullying_type'].unique()
plt.figure(figsize=(20, 10))

for i, label in enumerate(unique_labels, 1):
    # Combine all tweets of this class
    text = " ".join(df[df['cyberbullying_type'] == label]['tweet_text'].astype(str))

    # Generate word cloud
    wc = WordCloud(
        width=1000, height=600,
        background_color='white',
        colormap='coolwarm',
        max_words=150,
        contour_color='black',
        contour_width=2
    ).generate(text)

    # Plot
    plt.subplot(2, (len(unique_labels)+1)//2, i)
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"{label}", fontsize=20, fontweight='bold')

plt.suptitle("Word Cloud", fontsize=25, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

"""# Preprocessing"""

# ==========================================
#  Full Data Preprocessing Pipeline + Save (df Unchanged)
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from tqdm.notebook import tqdm
import pandas as pd

# Initialize tools
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

# -------------------------------------------------------
# Text Cleaning Function
# -------------------------------------------------------
def clean_text(text):
    if not isinstance(text, str):
        return ""

    # Lowercase
    text = text.lower()

    # Remove URLs, mentions, hashtags
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)
    text = re.sub(r"@\w+|#\w+", "", text)

    # Remove emojis & non-ASCII characters
    text = text.encode("ascii", "ignore").decode()

    # Remove punctuation and numbers
    text = re.sub(r"[^a-z\s]", " ", text)

    # Tokenize
    words = nltk.word_tokenize(text)

    # Remove stopwords and very short words
    words = [w for w in words if w not in stop_words and len(w) > 2]

    # Apply stemming
    words = [stemmer.stem(w) for w in words]

    return " ".join(words)

# -------------------------------------------------------
# Apply Preprocessing (on a copy)
# -------------------------------------------------------
print(" Initial shape of original df:", df.shape)

# Create a copy for preprocessing
preprocessed_df = df.copy()

# Drop missing or duplicate tweets
preprocessed_df = preprocessed_df.dropna(subset=['tweet_text', 'cyberbullying_type'])
preprocessed_df = preprocessed_df.drop_duplicates(subset=['tweet_text']).reset_index(drop=True)

# Drop unwanted class
preprocessed_df = preprocessed_df[preprocessed_df['cyberbullying_type'] != 'other_cyberbullying'].reset_index(drop=True)

print(" After removing nulls, duplicates, and unwanted class:", preprocessed_df.shape)

# Apply cleaning
tqdm.pandas()
preprocessed_df = preprocessed_df[preprocessed_df['tweet_text'].astype(str).str.strip() != ""]
preprocessed_df['cleaned_text'] = preprocessed_df['tweet_text'].progress_apply(clean_text)
preprocessed_df = preprocessed_df[preprocessed_df['cleaned_text'].astype(str).str.strip() != ""].reset_index(drop=True)

# -------------------------------------------------------
# Save Preprocessed Data
# -------------------------------------------------------
preprocessed_df.to_csv("/content/preprocessed_df.csv", index=False)

# -------------------------------------------------------
# Summary
# -------------------------------------------------------
print(" Text cleaning complete!")
print(f"Remaining samples: {len(preprocessed_df)}")
print(" Preprocessed data saved to: /content/preprocessed_df.csv")
print("\n Preview of Cleaned Data:")
print(preprocessed_df[['tweet_text', 'cyberbullying_type', 'cleaned_text']].head(10))

preprocessed_df

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Set up figure
unique_labels = preprocessed_df['cyberbullying_type'].unique()
plt.figure(figsize=(20, 10))

for i, label in enumerate(unique_labels, 1):
    # Combine all cleaned tweets of this class
    text = " ".join(preprocessed_df[preprocessed_df['cyberbullying_type'] == label]['cleaned_text'].astype(str))

    # Generate word cloud
    wc = WordCloud(
        width=1000, height=600,
        background_color='white',
        colormap='coolwarm',
        max_words=150,
        contour_color='black',
        contour_width=2
    ).generate(text)

    # Plot
    plt.subplot(2, (len(unique_labels) + 1) // 2, i)
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"{label}", fontsize=20, fontweight='bold', pad=15)

plt.suptitle("Word Cloud After Preprocessing", fontsize=25, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

# ==========================================
#  Dataset Size Comparison: Before vs After Preprocessing
# ==========================================

import matplotlib.pyplot as plt

# Count total rows before and after
before_count = len(df)
after_count = len(preprocessed_df)

print("  Dataset Size Comparison")
print("===========================")
print(f"Before Preprocessing : {before_count:,} rows")
print(f"After Preprocessing  : {after_count:,} rows")
print(f"Rows Removed         : {before_count - after_count:,} rows")

# -------------------------------
#  Bar Graph Comparison
# -------------------------------
plt.figure(figsize=(6, 5))
plt.bar(['Before Preprocessing', 'After Preprocessing'],
        [before_count, after_count],
        color=['skyblue', 'lightgreen'],
        width=0.5,
        edgecolor='black')

# Add value labels
for i, v in enumerate([before_count, after_count]):
    plt.text(i, v + 300, f"{v:,}", ha='center', fontweight='bold')

plt.title("Total Rows Before vs After Preprocessing", fontweight='bold')
plt.ylabel("Number of Rows")
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

"""# Feature Extraction - TF-IDF"""

# ==========================================
#  TF-IDF Feature Extraction
# ==========================================

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
import joblib

print(" Extracting TF-IDF features...")

# TF-IDF vectorization using cleaned text
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
X = vectorizer.fit_transform(preprocessed_df['cleaned_text'])

print(f" TF-IDF shape: {X.shape}")

# Label encode target
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(preprocessed_df['cyberbullying_type'])

print(" Label encoding complete!")
print(" Classes:", label_encoder.classes_)

# Save vectorizer and encoder
joblib.dump(vectorizer, "/content/tfidf_vectorizer.pkl")
joblib.dump(label_encoder, "/content/label_encoder.pkl")

# ==========================================
#  TF-IDF Feature Visualization
# ==========================================

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.decomposition import PCA

# -------------------------------------------------
# 1 View Top TF-IDF Words (Feature Importance)
# -------------------------------------------------
feature_names = vectorizer.get_feature_names_out()
tfidf_means = X.mean(axis=0).A1  # Mean TF-IDF value for each term

# Create DataFrame
tfidf_df = pd.DataFrame({
    'word': feature_names,
    'mean_tfidf': tfidf_means
}).sort_values(by='mean_tfidf', ascending=False).head(20)

# Plot top 20 TF-IDF words
plt.figure(figsize=(10,6))
bars = sns.barplot(y='word', x='mean_tfidf', data=tfidf_df)

# Add color gradient
bars.set_facecolor("none")
for patch, color in zip(bars.patches, sns.color_palette("coolwarm", len(tfidf_df))):
    patch.set_facecolor(color)

# Styling
plt.title("Top 20 Words by Mean TF-IDF Score", fontsize=14, fontweight='bold')
plt.xlabel("Mean TF-IDF Score", fontsize=12, fontweight='bold')
plt.ylabel("Word", fontsize=12, fontweight='bold')
plt.xticks(fontsize=10, fontweight='bold')
plt.yticks(fontsize=10, fontweight='bold')
plt.tight_layout()
plt.show()

# -------------------------------------------------
#  PCA Visualization of TF-IDF Features
# -------------------------------------------------
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X.toarray())

plt.figure(figsize=(10,6))
sns.scatterplot(
    x=X_pca[:,0], y=X_pca[:,1],
    hue=preprocessed_df['cyberbullying_type'],
    palette='tab10', alpha=0.7, s=60
)

plt.title("PCA Projection of TF-IDF Features", fontsize=18, fontweight='bold', pad=15)
plt.xlabel("Principal Component 1", fontsize=14, fontweight='bold')
plt.ylabel("Principal Component 2", fontsize=14, fontweight='bold')

#  Bigger and bold legend
legend = plt.legend(
    title="Cyberbullying Type",
    title_fontsize=13,
    fontsize=12,
    loc="best",
    frameon=True
)
for text in legend.get_texts():
    text.set_fontweight('bold')

plt.xticks(fontsize=12, fontweight='bold')
plt.yticks(fontsize=12, fontweight='bold')

plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

"""# Build Model - Logistic Regression"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, confusion_matrix, classification_report
)
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import time
import joblib

# -------------------------------
#  Split the dataset
# -------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f" Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}")

# -------------------------------
#  Train Logistic Regression model
# -------------------------------
print("\n Training Logistic Regression model...")
start_time = time.time()

model = LogisticRegression(max_iter=1000, solver='lbfgs', n_jobs=-1)
model.fit(X_train, y_train)

end_time = time.time()
training_time = end_time - start_time

# -------------------------------
#  Predictions & Evaluation
# -------------------------------
y_pred = model.predict(X_test)

print("\n Classification Report:")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

accuracy = accuracy_score(y_test, y_pred)
print(f" Test Accuracy: {accuracy * 100:.2f}%")
print(f" Training Time: {training_time:.2f} seconds")

# -------------------------------
#  Save the trained model
# -------------------------------
joblib.dump(model, "/content/cyberbullying_lr_model.pkl")

"""# Confusion Matrix"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# -------------------------------
# Confusion Matrix Visualization
# -------------------------------
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(10,7))
sns.heatmap(
    cm, annot=True, fmt='d', cmap='Reds',
    xticklabels=label_encoder.classes_,
    yticklabels=label_encoder.classes_,
    annot_kws={"size": 14, "weight": "bold"}  # Annotation font
)

# Title and Axis Labels
plt.title("Confusion Matrix", fontsize=18, fontweight='bold', pad=15)
plt.xlabel("Predicted Label", fontsize=14, fontweight='bold', labelpad=10)
plt.ylabel("True Label", fontsize=14, fontweight='bold', labelpad=10)

# Tick Labels
plt.xticks(fontsize=12, fontweight='bold', rotation=90, ha='right')
plt.yticks(fontsize=12, fontweight='bold', rotation=0)

plt.tight_layout()
plt.show()

"""# ROC Curve"""

# ==========================================
#  ROC Curve (with Macro & Micro Average)
# ==========================================
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

# Binarize output for multi-class ROC
y_test_bin = label_binarize(y_test, classes=np.unique(y))
n_classes = y_test_bin.shape[1]

# Get prediction probabilities
y_score = model.predict_proba(X_test)

# Compute ROC curve and ROC area for each class
fpr, tpr, roc_auc = {}, {}, {}
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# -------------------------------
#  Compute micro and macro-average ROC
# -------------------------------
# Micro-average (aggregate contributions of all classes)
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Macro-average (mean of all class ROC curves)
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
mean_tpr /= n_classes
fpr["macro"], tpr["macro"] = all_fpr, mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# -------------------------------
#  Plot ROC curves
# -------------------------------
plt.figure(figsize=(8,6))
# Class-wise ROC curves
for i in range(n_classes):
    plt.plot(
        fpr[i], tpr[i],
        lw=2,
        label=f"{label_encoder.classes_[i]} (AUC = {roc_auc[i]:.3f})"
    )

# Macro-average ROC
plt.plot(
    fpr["macro"], tpr["macro"],
    color='navy', linestyle=':', linewidth=3,
    label=f"Macro-average ROC (AUC = {roc_auc['macro']:.3f})"
)

# Micro-average ROC
plt.plot(
    fpr["micro"], tpr["micro"],
    color='deeppink', linestyle='--', linewidth=3,
    label=f"Micro-average ROC (AUC = {roc_auc['micro']:.3f})"
)

# Reference line
plt.plot([0, 1], [0, 1], 'k--', lw=2)

# Titles and labels
plt.title("ROC Curve", fontsize=16, fontweight='bold', pad=10)
plt.xlabel("False Positive Rate", fontsize=14, fontweight='bold')
plt.ylabel("True Positive Rate", fontsize=14, fontweight='bold')

# Customize ticks and legend
plt.xticks(fontsize=12, fontweight='bold')
plt.yticks(fontsize=12, fontweight='bold')
plt.legend(fontsize=11, loc="lower right", frameon=True)

plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

"""# Performance Metrics"""

# ==========================================
#  Model Performance Metrics
# ==========================================
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, matthews_corrcoef, confusion_matrix
)
import numpy as np
import time

# -------------------------------
#  Start Evaluation Timer
# -------------------------------
start_eval = time.time()

# Get predictions and probabilities
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)
end_eval = time.time()
computation_time = end_eval - start_eval

# -------------------------------
#  Confusion Matrix & Specificity
# -------------------------------
cm = confusion_matrix(y_test, y_pred)
specificities = []

for i in range(len(cm)):
    TP = cm[i, i]
    FN = np.sum(cm[i, :]) - TP
    FP = np.sum(cm[:, i]) - TP
    TN = np.sum(cm) - (TP + FP + FN)
    specificity_i = TN / (TN + FP + 1e-10)
    specificities.append(specificity_i)

specificity = np.mean(specificities)

# -------------------------------
#  Performance Metrics
# -------------------------------
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro', zero_division=0)
recall = recall_score(y_test, y_pred, average='macro', zero_division=0)
f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)
mcc = matthews_corrcoef(y_test, y_pred)

# Try ROC-AUC computation safely
roc_auc = None
try:
    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
except Exception:
    pass

# -------------------------------
#  Pretty Output
# -------------------------------
print("    Performance Metrics ")
print("============================")
print(f" Accuracy          : {accuracy:.4f}")
print(f" Precision         : {precision:.4f}")
print(f" Recall            : {recall:.4f}")
print(f" F1-score          : {f1:.4f}")
print(f" Specificity       : {specificity:.4f}")
print(f" MCC               : {mcc:.4f}")
if roc_auc is not None:
    print(f" ROC-AUC           : {roc_auc:.4f}")
print(f" Computation Time  : {computation_time:.4f} sec")
print("============================")

